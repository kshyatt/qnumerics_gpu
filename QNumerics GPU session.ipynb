{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all prerequisites on Google Colab\n",
    "# comment out this entire block with #= =# when it finishes!\n",
    "\n",
    "%%shell\n",
    "set -e\n",
    "\n",
    "#---------------------------------------------------#\n",
    "JULIA_VERSION=\"1.9.4\"\n",
    "JULIA_PACKAGES=\"IJulia StaticArrays\"\n",
    "JULIA_PACKAGES_IF_GPU=\"CUDA\"\n",
    "JULIA_NUM_THREADS=2\n",
    "#---------------------------------------------------#\n",
    "\n",
    "if [ -z `which julia` ]; then\n",
    "  # Install Julia\n",
    "  JULIA_VER=`cut -d '.' -f -2 <<< \"$JULIA_VERSION\"`\n",
    "  echo \"Installing Julia $JULIA_VERSION on the current Colab Runtime...\"\n",
    "  BASE_URL=\"https://julialang-s3.julialang.org/bin/linux/x64\"\n",
    "  URL=\"$BASE_URL/$JULIA_VER/julia-$JULIA_VERSION-linux-x86_64.tar.gz\"\n",
    "  wget -nv $URL -O /tmp/julia.tar.gz # -nv means \"not verbose\"\n",
    "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
    "  rm /tmp/julia.tar.gz\n",
    "\n",
    "  # Install Packages\n",
    "  nvidia-smi -L &> /dev/null && export GPU=1 || export GPU=0\n",
    "  if [ $GPU -eq 1 ]; then\n",
    "    JULIA_PACKAGES=\"$JULIA_PACKAGES $JULIA_PACKAGES_IF_GPU\"\n",
    "  fi\n",
    "  for PKG in `echo $JULIA_PACKAGES`; do\n",
    "    echo \"Installing Julia package $PKG...\"\n",
    "    julia -e 'using Pkg; pkg\"add '$PKG'; precompile;\"' &> /dev/null\n",
    "  done\n",
    "\n",
    "  # Install kernel and rename it to \"julia\"\n",
    "  echo \"Installing IJulia kernel...\"\n",
    "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\n",
    "      \"JULIA_NUM_THREADS\"=>\"'\"$JULIA_NUM_THREADS\"'\"))'\n",
    "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
    "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
    "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia  \n",
    "\n",
    "  echo ''\n",
    "  echo \"Successfully installed `julia -v`!\"\n",
    "  echo \"Please reload this page (press Ctrl+R, âŒ˜+R, or the F5 key)\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf20c80",
   "metadata": {},
   "source": [
    "# Hands-on with CUDA.jl\n",
    "\n",
    "In this notebook, we'll use [`CUDA.jl`](https://github.com/JuliaGPU/CUDA.jl) to control and program NVIDIA GPU resources. If you have an NVIDIA GPU already (in your laptop or desktop), you can run this notebook locally. If not, you can use a GPU available through [Google Colab](https://colab.research.google.com/). This will allow you to use a GPU *for free* for up to 12 hours of interactive experimentation.\n",
    "\n",
    "We'll use `CUDA.jl` because:\n",
    "- NVIDIA GPUs are highly available\n",
    "- CUDA is (for now) the most mature GPU programming paradigm with many libraries and resources\n",
    "- Many of the underlying concepts transfer to other systems like AMD ROCm and Intel oneAPI\n",
    "\n",
    "Let's import `CUDA.jl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c71e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb681d6",
   "metadata": {},
   "source": [
    "CUDA is actually made up of several components:\n",
    "- The GPU driver, which controls the graphics card and its interactions with the operating system kernel\n",
    "- The CUDA runtime, which provides the basic CUDA C extensions, CUDA API, and CUDA compiler\n",
    "- The CUDA libraries, which provide a \"GPUified\" implementation of BLAS (CUBLAS), RNG (CURAND), and many other things\n",
    "\n",
    "`CUDA.jl` has prepackaged [artifacts](https://pkgdocs.julialang.org/v1/artifacts/) for many CUDA versions, Julia versions, and OSes. It will download the appropriate one by default and install it for you. This lets us use the high-level Julia functions for CUDA arrays without worrying about the backend API calls (for now). Once `CUDA.jl` has installed, we can try creating some arrays on the GPU. We do this with `CuArray`, which turns a CPU array into a CUDA array by copying it.\n",
    "\n",
    "`CUDA.jl` also has some convenience constructors for `CuArray`s, like `CUDA.zeros`, `CUDA.rand`, and `undef` initializers. We can even use methods like `map` and `reduce` on these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe2a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_a = CUDA.rand(Float64, 1024, 1024);\n",
    "typeof(cu_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f2f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_b = map(x->x^2, cu_a);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3168a00c",
   "metadata": {},
   "source": [
    "These `CuArray`s live in **device memory** (GPU RAM), which means their individual elements are only available **from code executing on the GPU**. We know that the GPU memory is relatively limited compared to CPU memory, so how does `CUDA.jl` handle this? Do we have to explicitly \"reap\" these arrays ourselves?\n",
    "\n",
    "`CUDA.jl` extends the Julia garbage collector to work with GPU arrays as well. It manages a memory pool of GPU memory and garbage-collected `CuArray`s are in fact returned to the memory pool (rather than being entirely deallocated) in order to reduce GC pressure. `CUDA.jl` recycles arrays in the pool preferentially to speed up future allocations and keep an eye on how close we are to running out of memory. We can inspect the current state of the memory pool with `CUDA.pool_status()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5065b29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.pool_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f4311",
   "metadata": {},
   "source": [
    "Let's allocate another device array and see how the pool status changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_c = CUDA.rand(Float64, 1024, 1024)\n",
    "cu_c .+= cu_c' # make cu_c symmetric\n",
    "CUDA.pool_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9945587",
   "metadata": {},
   "source": [
    "We should never need to explicitly deallocate CUDA arrays, but we can reduce GC pressure and help CUDA.jl out by indicating when we are done with an array, using `CUDA.unsafe_free!`. As you can guess from the name, you should **never** refer to the array after you've unsafely freed it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.unsafe_free!(cu_b)\n",
    "CUDA.pool_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3c5d7",
   "metadata": {},
   "source": [
    "### Tips and tricks for higher level array operations\n",
    "\n",
    "In general, a `map` or `broadcast` over columns/slices launches multiple independent kernels which is usually slower than launching one larger kernel. For example,\n",
    "```julia\n",
    "broadcast(eachcol(a)) do x\n",
    "    prod(x)\n",
    "end\n",
    "```\n",
    "will usually be slower than `prod(a; dims=2)`.\n",
    "\n",
    "`CUDA.jl` isn't a tensor compiler and generally implements scalar (single-element) kernels for many operations like `map`. It's up to you to effectively use tools like broadcasting to avoid this.\n",
    "\n",
    "Broadcast fusion is especially helpful for GPU arrays because it turns multiple kernel launches on the same data into one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a53ff1c",
   "metadata": {},
   "source": [
    "## Using `LinearAlgebra` extensions\n",
    "\n",
    "Already we can see many of the nice array features we covered yesterday apply just as well to `CuArray`s. Many, but not all. `view`s, for example, are inconsistently supported. We can also make use of many wrapped linear algebra functions optimized for GPU via `CUBLAS`. We'll set the `CUBLAS` logger to prove to ourselves that it is in fact being used from our nice compact call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1658577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "CUBLAS.cublasLoggerConfigure(1, 0, 1, C_NULL) # normally we don't do this, just for demonstration purposes here\n",
    "cu_d = cu_c * cu_a;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1077083",
   "metadata": {},
   "source": [
    "We also have wrappers for some factorizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50faff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_c = qr(CUDA.rand(Float64, 64, 32));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9b8e2",
   "metadata": {},
   "source": [
    "## Running multiple functions simultaneously\n",
    "\n",
    "A `1024 x 1024` matrix is decently hefty but not big enough to saturate the throughput of the GPU. If we have many such matrices, we can spawn multiple separate simultaneous executions on the same GPU to speed up our computation, provided we don't run out of memory. `CUDA.jl` has a nice integration with the built-in Julia threading for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a217c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arrs = 100\n",
    "cpu_arrays = [rand(Float64, 1024, 1024) for ix_arr in 1:n_arrs]\n",
    "results = Vector{Float64}(undef, n_arrs)\n",
    "@sync begin\n",
    "    for ix_arr in 1:n_arrs\n",
    "        Threads.@spawn begin\n",
    "            results[ix_arr] = mapreduce(sin, *, CuArray(cpu_arrays[ix_arr]))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591d673",
   "metadata": {},
   "source": [
    "Obviously this example is a bit contrived, but using this basic pattern allows Julia and CUDA to work together to interleave memory copies and useful work, making better overall use of your GPU. We wrap everything in the `@sync` block because `Threads.@spawn` returns immediately after creating and launching its task, and `@sync` forces Julia to wait for all the `@spawn`-ed tasks to complete (and thus for all elements of `results` to be populated)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248de6e2",
   "metadata": {},
   "source": [
    "## Important considerations for memory transfers\n",
    "\n",
    "- Memory copies to/from/across the GPU are **high latency**. Each one takes a relatively long time to kick off. Thus, it's usually better to do **one large copy** (by concatenating arrays, for example) than **many small copies**.\n",
    "- By default, `CuArray`s live in the GPU memory and can't be accessed elementwise from the CPU. You, as the programmer, explicitly request memory copies, which helps you control and understand when they happen. However, you can use the CUDA [Unified Memory](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/) [from `CUDA.jl`](https://cuda.juliagpu.org/dev/usage/memory/#Unified-memory), which makes the arrays visible from both CPU and GPU. The CUDA driver copies memory back and forth for you as needed. This can be very convenient for the programmer but can lead to degraded performance depending on your memory access pattern.\n",
    "- If you have a pre-allocated CPU array you'll be copying to and from frequently (a pre-allocated out of loop buffer, for example), you can **pin** the CPU memory using `CUDA.pin`, which can speed up these copies substantially. The very act of pinning is itself time-consuming, so be sure it's worth it (benchmarks!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fabbef",
   "metadata": {},
   "source": [
    "## When the builtin wrappers aren't enough: writing our own kernels ðŸŒ½\n",
    "\n",
    "Sometimes the high level Julia constructs just aren't enough. We need to implement our own high-performance functions that will be executed across the massive thread population. Awesome! However, there are some new complications:\n",
    "- We have to make efficient use of the hardware, which isn't always intuitive\n",
    "- We have a restricted subset of the programming language available on-GPU\n",
    "- For many operations it's not obvious how to translate the algorithm to an efficient GPU implementation\n",
    "\n",
    "A good example of this latter point is creating an [alias table](https://github.com/LilithHafner/AliasTables.jl/) for discrete categorical sampling (this is a common thing to do for sampling state vectors, for example). Many CPU algorithms exist for this purpose, and some GPU ones too, but the underlying approach is quite different because of the GPU's constraints.\n",
    "\n",
    "A function we will execute in a massively/\"embarrassingly\" parallel way on the GPU across a large array is called a *kernel*. We can write a restricted subset of Julia within a kernel. `CUDA.jl` and its backend `GPUCompiler.jl` handle translating & compiling this through LLVM to the underlying GPU IR.\n",
    "\n",
    "The higher-level functions we worked with before are actually implemented behind-the-scenes as CUDA kernels. It's worth spending a little time to understand what is happening when a kernel executes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754f9a25",
   "metadata": {},
   "source": [
    "### Quick guide to kernel launches and execution\n",
    "\n",
    "CUDA spawns a *grid* of *blocks* of *threads*, and each thread is an individual worker. Threads are grouped into *warps* of size 32. Each thread in the warp steps through the function instructions **together**. If two threads in the same warp encounter **different** instructions, the warp can no longer execute in parallel and the performance benefit of using the GPU can disappear. This is called **warp divergence** (very *Star Trek*). In general, one warp out of 10,000 experiencing divergence will not hurt things too badly -- but a large fraction of warps experiencing it will.\n",
    "\n",
    "**Our goal when writing CUDA kernels is to make efficient use of warp-parallelism.**\n",
    "\n",
    "When we launch the CUDA kernel, we launch it with `(n_blocks_x, n_blocks_y, n_blocks_z)` blocks of `(n_threads_x, n_threads_y, n_threads_z)` threads. Any of these numbers can be 1 -- but you can also create 3D grids if it's more natural for your problem. In general, it's **not** the case that `n_blocks_x * n_blocks_y * n_blocks_z * n_threads_x * n_threads_y * n_threads_z` threads execute in parallel (though this can happen if the sizes are small). Rather, blocks are scheduled by the CUDA driver, so that block 0 may execute, and then block 65, then block 32. **Block execution order is not guaranteed!**\n",
    "\n",
    "Because the warps are size 32, it is most efficient to make the thread dimensions a power of 2. So, `n_threads_x = 64` would be a good choice, or `n_threads_y = 512`. **In general**, larger thread counts are better (512 threads-per-block is usually more performant than 64) but of course there are some edge cases.\n",
    "\n",
    "Within a kernel, each thread has access to some information about its personal location in the overall grid. We can query the `threadIdx()` and `blockIdx()` functions to get the thread's `x`, `y`, and `z` thread and block positions, and `blockDim()` to get the number of threads in a block in the `x`, `y`, and `z` directions.\n",
    "\n",
    "We start by writing a `function` which may take arguments. `CUDA.jl` kernels aren't yet hooked up to standard Julia IO, but we **can** do some basic printing using the `@cuprintln` macro. **CUDA kernels must always return `nothing`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function my_first_cuda_kernel(a)\n",
    "    t_x = threadIdx().x\n",
    "    b_x = blockIdx().x\n",
    "    l_a = length(a)\n",
    "    @cuprintln(\"Hi! My thread index is $t_x and my block index is $b_x. The length of my argument is $l_a.\")\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7a047",
   "metadata": {},
   "source": [
    "Now we *launch* the kernel on the GPU using the `@cuda` macro. We can specify how many threads and blocks to launch with at this time. **Keep in mind that CUDA kernels cannot see arrays in CPU memory, only GPU arrays.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d53524",
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_a = CUDA.rand(Float32, 512)\n",
    "\n",
    "@cuda threads=64 blocks=div(length(cu_a), 64) my_first_cuda_kernel(cu_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa79ca",
   "metadata": {},
   "source": [
    "However, **stack-allocated objects** like scalar integers, floats, or *tuples* (and thus *StaticArrays*) **can** be passed as arguments to CUDA kernels. ðŸ˜ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eee3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "using StaticArrays\n",
    "\n",
    "static_a = SVector{16, ComplexF64}(rand(ComplexF64) for si in 1:16)\n",
    "@cuda threads=16 my_first_cuda_kernel(static_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e1473b",
   "metadata": {},
   "source": [
    "### Specifying type constraints in CUDA kernel arguments\n",
    "\n",
    "From the point of view of a CUDA kernel, a `CuArray{T}` is a `CuDeviceArray{T}`. This is a new type that implements GPU-compatible operations, and the conversion is handled automatically for you. You can use multiple dispatch to define different kernels for different element types or array dimension and call the appropriate kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a78f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function my_first_cuda_kernel(a::CuDeviceArray{Float32})\n",
    "    t_x = threadIdx().x\n",
    "    b_x = blockIdx().x\n",
    "    @cuprintln(\"Hi! My thread index is $t_x and my block index is $b_x. Wowow Float32 so speedy.\")\n",
    "    return\n",
    "end\n",
    "\n",
    "function my_first_cuda_kernel(a::CuDeviceArray{Float16})\n",
    "    t_x = threadIdx().x\n",
    "    b_x = blockIdx().x\n",
    "    @cuprintln(\"Hi! My thread index is $t_x and my block index is $b_x. Float16 so slender.\")\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@sync @cuda threads=16 my_first_cuda_kernel(CUDA.rand(Float32, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@sync @cuda threads=16 my_first_cuda_kernel(CUDA.rand(Float16, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e650d08a",
   "metadata": {},
   "source": [
    "## Avoiding warp divergence\n",
    "\n",
    "As mentioned above, threads in the same warp executing different instructions forces each thread to execute in serial, dramatically slowing execution. So does this mean we can never use `if` or `else`? Let's do some experiments to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d05dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "function diverging_kernel(arr)\n",
    "    my_flag = arr[threadIdx().x]\n",
    "    a = 1.0\n",
    "    n_iter = my_flag > 0 ? (my_flag + 1)*5 : 20\n",
    "    for ix in 1:n_iter\n",
    "        if my_flag > 0\n",
    "            a *= -2 * sin(rand() * Ï€/4)\n",
    "        else\n",
    "            a += sqrt(0.25)\n",
    "        end\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b654f32",
   "metadata": {},
   "source": [
    "Let's try launching and timing this with various values for `arr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293787a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda threads=64 diverging_kernel(CUDA.zeros(Int, 64));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b96d8-9923-4fc0-b365-6c3c57856312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cu_arr = CuArray(rand(0:1, 64))\n",
    "CUDA.@profile @cuda threads=64 blocks=50_000 diverging_kernel(cu_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_arr = CuArray(vcat(zeros(Int, 32), ones(Int, 32)))\n",
    "CUDA.@profile @cuda threads=64 blocks=50_000 diverging_kernel(cu_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb220e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_arr = CUDA.ones(Int, 64)\n",
    "CUDA.@profile @cuda threads=64 blocks=50_000 diverging_kernel(cu_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_arr = CuArray(rand(-1:3, 64))\n",
    "CUDA.@profile @cuda threads=64 blocks=50_000 diverging_kernel(cu_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21bbb1f",
   "metadata": {},
   "source": [
    "### Forbidden operations\n",
    "\n",
    "Not all valid Julia code is valid inside a CUDA kernel. In particular, within a kernel we **cannot use**:\n",
    "- `String`s, except inside a `@cuprint`/`@cuprintln`/`@cuprintf` macro call\n",
    "- `Exception`s\n",
    "- Allocations (with two exceptions)\n",
    "\n",
    "Additionally, type-unstable or uninferrable code may not be compilable and may generate long and sometimes cryptic errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd2515",
   "metadata": {},
   "source": [
    "## Working with memory inside kernels\n",
    "\n",
    "When a warp executes, it has access to 3 types of memory:\n",
    "- Global memory, or the on-device GPU RAM. This is the largest and slowest memory.\n",
    "- Local memory, which holds locally defined variables for each thread.\n",
    "- Shared memory, which is shared (wow, really?) among all the threads in the warp. This memory allows threads **to communicate with one another**.\n",
    "\n",
    "Shared memory is physically located next to the cores executing the code and is fastest. To allocate shared memory, we can either define a `CuStaticSharedArray(T, N)`, with `N` elements of type `T` (`N` must be known at compile time) or a `CuDynamicSharedArray(T, N)`, where `N` isn't known at compile-time but will be passed to the kernel at *launch* time. **Note** that when you're using shared memory, you should call `sync_threads()` after any updates to it to make sure all threads in the warp can \"see\" the update (avoiding a data race)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ceaf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function reverse_kernel(a::CuDeviceArray{T}) where T\n",
    "    i = threadIdx().x\n",
    "    b = CuDynamicSharedArray(T, length(a))\n",
    "    b[length(a)-i+1] = a[i]\n",
    "    sync_threads()\n",
    "    a[i] = b[i]\n",
    "    return\n",
    "end\n",
    "\n",
    "cu_a = CuArray([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "@cuda threads=length(cu_a) shmem=sizeof(cu_a) reverse_kernel(cu_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd92ba1",
   "metadata": {},
   "source": [
    "The `shmem` argument should be the *total length* of shared memory needed across *all threads*. Many advanced GPU algorithms use shared memory for their effectiveness, so it's worth being aware of. The above example was pretty small -- do you think this would work if `a` were very large? Would we have to modify our kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806498ea",
   "metadata": {},
   "source": [
    "### Memory access patterns\n",
    "\n",
    "As we discussed yesterday, the order in which we access array elements can have big performance impacts. In general, accessing adjacent memory locations in GPU memory is **much** faster than accessing locations that are widely separated. This is because to read a single location, the thread(s) have to load a whole \"word\" of memory at a time, so two memory locations that are part of the same word can be read cheaply once the word is loaded. This is also sometimes called \"coalesced\" memory access. It's most efficient to align warp-reads with 32-byte boundary memory regions -- something that using *strides* can often help with! How much of an impact can this have? For a matrix-matrix multiplication, NVIDIA ran tests on a V100 GPU. The observed memory bandwith was:\n",
    "\n",
    "- Naive implementation: `12.8 GB/s`\n",
    "- With shared memory to coalesce reads from global memory: `140.2 GB/s`\n",
    "- Further optimizations from reduce bank conflicts: `199.4 GB/s`\n",
    "\n",
    "Data taken from the [CUDA C++ programming guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory). So, if your application involves a lot of reading and writing to memory, it's really worth figuring out how to use shared memory effectively. This is also something the profiler can help with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c2413",
   "metadata": {},
   "source": [
    "## Benchmarking and profiling\n",
    "\n",
    "`CUDA.jl` comes with a great integration with the NVIDIA CUDA profilers. This makes it easier to profile our kernels and find and fix performance problems. We can use the `CUDA.@profile` macro to see where we're spending our time. Let's revisit our threading example from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arrs = 100\n",
    "CUDA.@profile begin\n",
    "    cpu_arrays = [rand(Float64, 1024, 1024) for ix_arr in 1:n_arrs]\n",
    "    results = Vector{Float64}(undef, n_arrs)\n",
    "    @sync begin\n",
    "        for ix_arr in 1:n_arrs\n",
    "            Threads.@spawn begin\n",
    "                results[ix_arr] = mapreduce(sin, *, CuArray(cpu_arrays[ix_arr]))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df4d5f4",
   "metadata": {},
   "source": [
    "We can also generate a trace to provide to NVIDIA's profile visualizer, which can give us lots of detailed information and tips about what to fix. We can either launch Julia [inside the `nsys` profiler](https://cuda.juliagpu.org/dev/development/profiling/#NVIDIA-Nsight-Systems) or generate the file by setting the `external=true` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1a4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arrs = 100\n",
    "CUDA.@profile external=true begin\n",
    "    cpu_arrays = [rand(Float64, 1024, 1024) for ix_arr in 1:n_arrs]\n",
    "    results = Vector{Float64}(undef, n_arrs)\n",
    "    @sync begin\n",
    "        for ix_arr in 1:n_arrs\n",
    "            Threads.@spawn begin\n",
    "                results[ix_arr] = mapreduce(sin, *, CuArray(cpu_arrays[ix_arr]))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb31844",
   "metadata": {},
   "source": [
    "You can then open the generated report file in NSight Systems and see a trace of what the GPU was doing when, how much load it was under, and use this information to improve your code (e.g. by overlapping kernel launches and memory copies). You can also use [NVIDIA NSight Compute](https://cuda.juliagpu.org/dev/development/profiling/#NVIDIA-Nsight-Compute) to do very detailed analysis on a single kernel and pinpoint things to fix.\n",
    "\n",
    "**In GPU programming, you absolutely need to benchmark your kernels and use the profiler to fix any issues.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beff884f",
   "metadata": {},
   "source": [
    "## More resources\n",
    "\n",
    "A lot of the tutorials and resources for CUDA and GPU programming in general are written in C or C++. Many of the concepts apply to writing performant kernels in Julia or any other language. Additionally, some older references don't account for new device features (like tensor cores) that NVIDIA has introduced over the years. The best technique is to implement something and profile it!\n",
    "\n",
    "- [CUDA C Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide)\n",
    "- [JuliaCon 2021 GPU workshop](https://github.com/maleadt/juliacon21-gpu_workshop) -- 3 hours worth of tutorial content\n",
    "- [`CUDA.jl` documentation](https://cuda.juliagpu.org/dev/)\n",
    "- [`KernelAbstractions.jl`](https://juliagpu.github.io/KernelAbstractions.jl/dev/) -- write once, run on any GPU\n",
    "- [`DaggerGPU.jl`](https://github.com/JuliaGPU/DaggerGPU.jl) -- graph-based scheduler that can take GPU resources into account\n",
    "- [Hands on with Julia for HPC on GPUs and CPUs](https://www.youtube.com/watch?v=RNmSqbG2MUc)\n",
    "- [Differentiable modeling on GPUs workshop](https://github.com/PTsolvers/gpu-workshop-JuliaCon23)\n",
    "- [`#gpu` channel on Julia slack](https://julialang.org/slack/)\n",
    "- [GPU category on Julia discourse](https://discourse.julialang.org/c/domain/gpu/11)\n",
    "- [JuliaGPU Zoom office hours](https://julialang.org/community/), scroll down for the calendar invite"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
